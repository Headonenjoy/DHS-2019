{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHPyjYbK95K9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import codecs\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yryBnBU7-We1",
    "outputId": "95727656-74ae-4c9f-ab09-2b2d598ddc87",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.models.doc2vec.FAST_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdHHRIEH-Z4O",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Generate list of sentences\n",
    "class MySentences(object):\n",
    "    def __init__(self, fileName):\n",
    "        self.fileName = fileName\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in codecs.open(filename=self.fileName, encoding='utf-8'):\n",
    "            yield line[:-1].split('\",\"')\n",
    "\n",
    "#Save model after each Epoch\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    # Callback to save model after each epoch.\n",
    "    def __init__(self, path_prefix):\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        output_path = '{}_epoch{}.model'.format(self.path_prefix, self.epoch)\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1\n",
    "\n",
    "#Custom Logging\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    # Callback to log information about training\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgcNeRygiwvc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "WORD2VEC_MODEL = 'skip-gram'\n",
    "WALKS_PATH = '~/Desktop/dataHackSummit/SearchTermRecommendations/termlist.txt'\n",
    "WORD2VEC_EPOCH_SAVE_PATH = '~/Desktop/dataHackSummit/SearchTermRecommendations/{}/epoch/word2vec'.format(WORD2VEC_MODEL)\n",
    "FINAL_MODEL_PATH = '~/Desktop/dataHackSummit/SearchTermRecommendations/{}/word2vec{}.model'.format(WORD2VEC_MODEL, WORD2VEC_MODEL)\n",
    "FINAL_VECTOR_PATH = '~/Desktop/dataHackSummit/SearchTermRecommendations/{}/wordvectors-{}.kv'.format(WORD2VEC_MODEL, WORD2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BUxpoZP-dEC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = MySentences(WALKS_PATH)\n",
    "epochSaver = EpochSaver(WORD2VEC_EPOCH_SAVE_PATH)\n",
    "epochLogger = EpochLogger()\n",
    "callbacks = [epochSaver, epochLogger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "var_size = 100 #Size of the embeddings. Usually 100-300 does the trick.\n",
    "var_window = 7 #Max length of our sequences is 15 hence everything will be covered.\n",
    "var_min_count = 100 #Minimum occurence of search terms\n",
    "var_sg = 1 # 1 for skip gram 0 for cbow. skip gram gives good vectors for infrequent words.\n",
    "var_num_iter = 30 # After 25-30 iterations we do not see any major changes in the vectors\n",
    "# There are other parameters also like hierarchical softmax -\n",
    "#It is needed when our num of words are more(estimates the costly softmax fn) Not needed in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AovuefkT-pVI",
    "outputId": "5de454e4-9be5-40a2-aa32-d8e0bb25a4a5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences = sentences,\n",
    "                 size=var_size, window=var_window,\n",
    "                 min_count=var_min_count, sg=var_sg,\n",
    "                 workers=8, seed=1234,\n",
    "                 iter=var_num_iter, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "bzHA2EJlCUcw",
    "outputId": "10ec3e0a-4a29-462e-d934-fb338830a840",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lager', 0.7699299454689026),\n",
       " ('vodka', 0.7030935287475586),\n",
       " ('gin', 0.6884914040565491),\n",
       " ('wine', 0.6875373125076294),\n",
       " ('cider', 0.6839544773101807),\n",
       " ('corona', 0.6719756126403809),\n",
       " ('budweiser', 0.6610127091407776),\n",
       " ('white wine', 0.6550659537315369),\n",
       " ('prosecco', 0.6474155783653259),\n",
       " ('rose wine', 0.6439282298088074)]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('beer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "CWGgeBO-KzyC",
    "outputId": "3380da5a-43ca-4fc4-9e32-8621c43e0bb4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('noodles', 0.0029194446),\n",
       " ('fish', 0.002858851),\n",
       " ('curry sauce', 0.002659693),\n",
       " ('soup', 0.0024856278),\n",
       " ('pasta sauce', 0.0024547083),\n",
       " ('salmon', 0.0024233577),\n",
       " ('meatball', 0.002419044),\n",
       " ('peas', 0.0023551204),\n",
       " ('curry', 0.002312012),\n",
       " ('fajita', 0.0022828763),\n",
       " ('rice', 0.0021993315),\n",
       " ('beef', 0.0021332463),\n",
       " ('steak', 0.0021258784),\n",
       " ('stir fry', 0.0021248306),\n",
       " ('veg', 0.0021211605),\n",
       " ('vegetables', 0.0021107865),\n",
       " ('yorkshire puddings', 0.0020970942),\n",
       " ('pork', 0.0020772126),\n",
       " ('beans', 0.002056384),\n",
       " ('mash', 0.002048504)]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word(['chicken','spice','soup', 'vegetable'], topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "_QnAsRDwK5kU",
    "outputId": "8678fd2c-2155-48ea-98ac-4f32069a32ff",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.wv.save(FINAL_VECTOR_PATH)\n",
    "model.save(FINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trying Harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.metrics import top_k_categorical_accuracy, sparse_top_k_categorical_accuracy\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "WORD2VEC_MODEL = 'skip-gram'\n",
    "DROPOUT = 0.5 #Dropout Percentage\n",
    "NEPOCHS = 50 # No of epochs to train RNN\n",
    "LAYERS = 2 # No of layers in RNN\n",
    "TRAD_DROPOUT = 0.1 # Traditional Dropout\n",
    "RECUR_DROPOUT = 0.5 # Reccurent Dropout\n",
    "NVOCAB  = 10000 # No of words in the Vocabulary \n",
    "EMBEDDING_DIM = 400 #Length of word2vec dimention\n",
    "NHIDDEN = 300   # size of hidden layer(s) \n",
    "N = 5        # Length of Sequence to RNN [Input + Output]\n",
    "RNN_CLASS = LSTM # type of RNN to use - SimpleRNN, LSTM, or GRU\n",
    "BATCH_SIZE = 256 #size of batch to use for training\n",
    "PATIENCE = 10        # stop after this many epochs of no improvement                    \n",
    "VALIDATION_SPLIT = 0.01 # percent of training data to use for validation (0.01 ~10k tokens)\n",
    "LOSS_FN    = 'sparse_categorical_crossentropy' # Loss Function used for trainning\n",
    "OPTIMIZER        = 'adam'    # optimizing algorithm to use (sgd, rmsprop, adam, adagrad, adadelta, adamax, nadam)  \n",
    "INITIALIZER      = 'uniform' # random weight initializer (uniform, normal, lecun_uniform, glorot_uniform [default])\n",
    "SEED             = 0         # random number seed                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MATRIX_DIR = '/content/drive/My Drive/SearchTermRecommendations/Sequence1/embeddingMatrix.npy'\n",
    "X_TRAIN_DIR = '/content/drive/My Drive/SearchTermRecommendations/Sequence/inputSequence_Train.npy'\n",
    "Y_TRAIN_DIR = '/content/drive/My Drive/SearchTermRecommendations/Sequence/outputSequence_Train.npy'\n",
    "X_TEST_DIR = '/content/drive/My Drive/SearchTermRecommendations/Sequence/inputSequence_Test.npy'\n",
    "Y_TEST_DIR = '/content/drive/My Drive/SearchTermRecommendations/Sequence/outputSequence_Test.npy'\n",
    "MODEL_SAVE_FILE_PATH = '/content/drive/My Drive/SearchTermRecommendations/RNNModel_2_SKIPGRAM_1024/model-{epoch:02d}-{val_top20_acc:.2f}.hdf5'\n",
    "CSV_LOGGER_SAVE_FILE_PATH = '/content/drive/My Drive/SearchTermRecommendations/ModelTrainningLogs/rnn.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "embeddingMatrix = np.load(EMBEDDING_MATRIX_DIR)\n",
    "X_TRAIN = np.load(X_TRAIN_DIR)\n",
    "Y_TRAIN = np.load(Y_TRAIN_DIR)\n",
    "X_TEST = np.load(X_TEST_DIR)\n",
    "Y_TEST = np.load(Y_TEST_DIR)\n",
    "NVOCAB,EMBEDDING_DIM = embeddingMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Build Model                                                                     \n",
    "# --------------------------------------------------------------------------------\n",
    "                                                                                  \n",
    "model = Sequential()                                                              \n",
    "                                                                                  \n",
    "# embedding layer                                                                 \n",
    "embedding_layer = Embedding(input_dim=NVOCAB, output_dim=EMBEDDING_DIM, input_length=N-1, weights=[embeddingMatrix])                        \n",
    "model.add(embedding_layer)\n",
    "\n",
    "if LAYERS==1:                                                             \n",
    "    model.add(RNN_CLASS(NHIDDEN, kernel_initializer=INITIALIZER, dropout = TRAD_DROPOUT, recurrent_dropout = RECUR_DROPOUT))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "                                           \n",
    "elif LAYERS==2:                                                           \n",
    "    model.add(RNN_CLASS(NHIDDEN, kernel_initializer=INITIALIZER, dropout = TRAD_DROPOUT, recurrent_dropout = RECUR_DROPOUT, return_sequences=True))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN, kernel_initializer=INITIALIZER, dropout = TRAD_DROPOUT, recurrent_dropout = RECUR_DROPOUT))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "    \n",
    "elif LAYERS==3:\n",
    "    model.add(RNN_CLASS(NHIDDEN, init=INITIALIZER, dropout = TRAD_DROPOUT, recurrent_dropout = RECUR_DROPOUT, return_sequences=True))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN, init=INITIALIZER, dropout = TRAD_DROPOUT, recurrent_dropout = RECUR_DROPOUT, return_sequences=True))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN, init=INITIALIZER, dropout = TRAD_DROPOUT, recurrent_dropout = RECUR_DROPOUT))\n",
    "    #model.add(Dropout(DROPOUT))\n",
    "    \n",
    "\n",
    "# output layer - convert nhidden to nvocab\n",
    "model.add(Dense(NVOCAB-1)) # The padded vocab not included in output layer\n",
    "\n",
    "# convert nvocab to probabilities - expensive \n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model ~ 1 sec\n",
    "top10_acc = functools.partial(sparse_top_k_categorical_accuracy, k=10)\n",
    "top10_acc.__name__ = 'top10_acc'\n",
    "top20_acc = functools.partial(sparse_top_k_categorical_accuracy, k=20)\n",
    "top20_acc.__name__ = 'top20_acc'\n",
    "metrics = [top10_acc, top20_acc] # loss is always the first metric returned from the fit method\n",
    "model.compile(loss=LOSS_FN, optimizer=OPTIMIZER, metrics=metrics)\n",
    "\n",
    "# Define the callbacks that will be required for fitting the model\n",
    "early_stopping = EarlyStopping(monitor='top20_acc', patience=PATIENCE)\n",
    "checkpoint = ModelCheckpoint(MODEL_SAVE_FILE_PATH, monitor='top20_acc', save_best_only=True, mode='max')\n",
    "csvlogger = CSVLogger(CSV_LOGGER_SAVE_FILE_PATH)\n",
    "callbacks = [checkpoint, early_stopping, csvlogger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gmhZirdSQsM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the callbacks that will be required for fitting the model\n",
    "early_stopping = EarlyStopping(monitor='top20_acc', patience=PATIENCE)\n",
    "checkpoint = ModelCheckpoint(MODEL_SAVE_FILE_PATH, monitor='top20_acc', save_best_only=True, mode='max')\n",
    "csvlogger = CSVLogger(CSV_LOGGER_SAVE_FILE_PATH)\n",
    "callbacks = [checkpoint, early_stopping, csvlogger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "28TdIEJZ951H",
    "outputId": "86516d5a-196c-4a6f-a588-a84de4d7c807",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6732799 samples, validate on 68009 samples\n",
      "Epoch 1/50\n",
      "6732799/6732799 [==============================] - 662s 98us/step - loss: 4.6838 - top10_acc: 0.3846 - top20_acc: 0.4810 - val_loss: 4.5808 - val_top10_acc: 0.3993 - val_top20_acc: 0.4972\n",
      "Epoch 2/50\n",
      "6732799/6732799 [==============================] - 660s 98us/step - loss: 4.5659 - top10_acc: 0.4025 - top20_acc: 0.5018 - val_loss: 4.5590 - val_top10_acc: 0.4038 - val_top20_acc: 0.5025\n",
      "Epoch 3/50\n",
      "6732799/6732799 [==============================] - 659s 98us/step - loss: 4.5403 - top10_acc: 0.4065 - top20_acc: 0.5066 - val_loss: 4.5500 - val_top10_acc: 0.4045 - val_top20_acc: 0.5044\n",
      "Epoch 4/50\n",
      "6732799/6732799 [==============================] - 761s 113us/step - loss: 4.5242 - top10_acc: 0.4087 - top20_acc: 0.5095 - val_loss: 4.5461 - val_top10_acc: 0.4065 - val_top20_acc: 0.5051\n",
      "Epoch 5/50\n",
      "6732799/6732799 [==============================] - 859s 128us/step - loss: 4.5117 - top10_acc: 0.4105 - top20_acc: 0.5115 - val_loss: 4.5484 - val_top10_acc: 0.4061 - val_top20_acc: 0.5045\n",
      "Epoch 6/50\n",
      "2482176/6732799 [==========>...................] - ETA: 8:18 - loss: 4.4944 - top10_acc: 0.4126 - top20_acc: 0.5140"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(X_TRAIN, Y_TRAIN, batch_size=BATCH_SIZE, \n",
    "                    epochs=NEPOCHS,validation_split=VALIDATION_SPLIT, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "WordEmdedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
